{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Support Vector Machine - Image Classification\n",
    "\n",
    "\n",
    "In this notebook we will train SVM classifiers to classify images. \n",
    "\n",
    "For a comparative understanding, we will compare the performance of the SVM with the Logistic Regression Softmax classifier.\n",
    "\n",
    "We will use dimensionality reduction technique (Principle Component Analysis) to project the features into a smaller dimension to expedite the training time.\n",
    "\n",
    "\n",
    "Generally **images are linearly non-separable**. Based on this we formulate the following hypotheses:\n",
    "- The kernelized SVM models will perform significantly better than the linear SVM model.\n",
    "- The RBF Kernel based SVM will perform better than Softmax regression classifier.\n",
    "- Dimensionaly reduction (by retaining maximum variance) should improve the performance.\n",
    "\n",
    "We will investige these hypotheses by conducting the following experiments.\n",
    "\n",
    "\n",
    "## Experiments\n",
    "\n",
    "- Experiment 1: Support Vector Machine (LinearSVC) + PCA\n",
    "- Experiment 2: Support Vector Machine (SVC with RBF Kernel) + PCA\n",
    "- Experiment 3: Support Vector Machine (SVC with RBF Kernel) \n",
    "- Experiment 4: Logistic Regression (Softmax Regression) + PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset: MNIST\n",
    "\n",
    "\n",
    "We will use the MNIST dataset, which is a set of 70,000 small images of digits handwritten by high school students and employees of the US Census Bureau. Each image is labeled with the digit it represents.\n",
    "\n",
    "\n",
    "There are 70,000 images. Each image is 28x28 pixels, and each feature simply represents one pixelâ€™s intensity, from 0 (white) to 255 (black).\n",
    "\n",
    "Thus, each image has 784 features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data and Create Data Matrix (X) and the Label Vector (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 784)\n",
      "(70000,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hasan/anaconda/lib/python3.5/site-packages/sklearn/utils/deprecation.py:77: DeprecationWarning: Function fetch_mldata is deprecated; fetch_mldata was deprecated in version 0.20 and will be removed in version 0.22\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "/Users/hasan/anaconda/lib/python3.5/site-packages/sklearn/utils/deprecation.py:77: DeprecationWarning: Function mldata_filename is deprecated; mldata_filename was deprecated in version 0.20 and will be removed in version 0.22\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "mnist = fetch_mldata('MNIST original')\n",
    "\n",
    "X, y = mnist[\"data\"], mnist[\"target\"] \n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data Into Training and Test Sets\n",
    "\n",
    "The MNIST dataset is already split into a training set (the first 60,000 images) and a test set (the last 10,000 images).\n",
    "\n",
    "We will shuffle the training set to ensure that all cross-validation folds will be similar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]\n",
    "\n",
    "shuffle_index = np.random.permutation(60000)\n",
    "X_train, y_train = X_train[shuffle_index], y_train[shuffle_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization Using Dimensionaly Reduction\n",
    "\n",
    "We can optimize the running-time of the Logistic Regression algorithm by reducing the number of features. Our assumption is that the essence or core content of the data does not span along all dimensions. The technique for reducing the dimension of data is known as dimensionality reduction.\n",
    "\n",
    "For a gentle introduction to various dimensionality reduction technique, see the notebook \"Dimensionality Reduction\" in the Github repository.\n",
    "\n",
    "We will use the Principle Component Analysis (PCA) dimensionality reduction technique to project the MNIST dataset (784 features) to a lower dimensional space by retaining maximum variance. \n",
    "\n",
    "The goal is to see the improvement in training time due to this dimensionality reduction.\n",
    "\n",
    "Before we apply the PCA, we need to standardize the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardize the Data\n",
    "\n",
    "PCA is influenced by scale of the data. Thus we need to scale the features of the data before applying PCA. \n",
    "\n",
    "For understanding the negative effect of not scaling the data, see the following post:\n",
    "\n",
    "https://scikit-learn.org/stable/auto_examples/preprocessing/plot_scaling_importance.html#sphx-glr-auto-examples-preprocessing-plot-scaling-importance-py\n",
    "\n",
    "Note that we fit the scaler on the training set and transform on the training and test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hasan/anaconda/lib/python3.5/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype uint8 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/Users/hasan/anaconda/lib/python3.5/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype uint8 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/Users/hasan/anaconda/lib/python3.5/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype uint8 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit on training set only.\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# Apply transform to both the training set and the test set.\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply PCA\n",
    "\n",
    "While applying PCA we can set the number of principle components by the \"n_components\" attribute. But more importantly, we can use this attribute to determine the % of variance we want to retain in the extracted features.\n",
    "\n",
    "For example, if we set it to 0.95, sklearn will choose the **minimum number of principal components** such that 95% of the variance is retained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22.2 s, sys: 1.69 s, total: 23.9 s\n",
      "Wall time: 7.77 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pca = PCA(n_components=0.95)\n",
    "\n",
    "pca.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of Principle Components\n",
    "\n",
    "We can find how many components PCA chose after fitting the model by using the following attribute: n_components_\n",
    "\n",
    "We will see that 95% of the variance amounts to **315 principal components**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numberof Principle Components:  331\n"
     ]
    }
   ],
   "source": [
    "print(\"Numberof Principle Components: \", pca.n_components_)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply the Mapping (Transform) to both the Training Set and the Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_pca = pca.transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments\n",
    "\n",
    "We will conduct the following experiments.\n",
    "\n",
    "- Experiment 1: Support Vector Machine (LinearSVC) + PCA\n",
    "- Experiment 2: Support Vector Machine (SVC with RBF Kernel) + PCA\n",
    "- Experiment 3: Support Vector Machine (SVC with RBF Kernel) \n",
    "- Experiment 4: Logistic Regression (Softmax Regression) + PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine: Model Selection via Hyperparameter Tuning\n",
    "\n",
    "Note that we are not performing grid search (which we should have). \n",
    "\n",
    "We are simply using the best values for the two hyperparameters ($\\gamma$ and $C$) for the SVC from prior grid search. However, it is advised that one should perform grid search to fine tune the hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1: LinearSVC + PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 32s, sys: 580 ms, total: 3min 33s\n",
      "Wall time: 3min 33s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hasan/anaconda/lib/python3.5/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "linear_svc_pca = LinearSVC(loss='hinge', C=1, random_state=42)\n",
    "linear_svc_pca.fit(X_train_pca, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1: Evaluate LinearSVC + PCA on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Accuracy:  0.918\n",
      "\n",
      "Test Confusion Matrix:\n",
      "[[ 957    0    1    2    0    4   10    3    2    1]\n",
      " [   0 1112    3    1    0    2    4    1   12    0]\n",
      " [  10    5  912   19    9    7   11   11   44    4]\n",
      " [   4    0   17  928    1   19    3   10   18   10]\n",
      " [   1    2    6    1  905    1   10    5    9   42]\n",
      " [   7    3    0   39   12  768   21    9   27    6]\n",
      " [  10    3    6    3    7   10  914    1    4    0]\n",
      " [   1   11   23   10   11    2    1  944    2   23]\n",
      " [  10    8    8   24   12   27    8   13  856    8]\n",
      " [  11    8    3   15   40   12    1   24   11  884]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      0.98      0.96       980\n",
      "         1.0       0.97      0.98      0.97      1135\n",
      "         2.0       0.93      0.88      0.91      1032\n",
      "         3.0       0.89      0.92      0.90      1010\n",
      "         4.0       0.91      0.92      0.91       982\n",
      "         5.0       0.90      0.86      0.88       892\n",
      "         6.0       0.93      0.95      0.94       958\n",
      "         7.0       0.92      0.92      0.92      1028\n",
      "         8.0       0.87      0.88      0.87       974\n",
      "         9.0       0.90      0.88      0.89      1009\n",
      "\n",
      "   micro avg       0.92      0.92      0.92     10000\n",
      "   macro avg       0.92      0.92      0.92     10000\n",
      "weighted avg       0.92      0.92      0.92     10000\n",
      "\n",
      "CPU times: user 115 ms, sys: 25.1 ms, total: 140 ms\n",
      "Wall time: 38.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "y_test_predicted = linear_svc_pca.predict(X_test_pca)\n",
    "\n",
    "accuracy_score_test = np.mean(y_test_predicted == y_test)\n",
    "print(\"\\nTest Accuracy: \", accuracy_score_test)\n",
    "\n",
    "print(\"\\nTest Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_test_predicted))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_test_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2: SVC (RBF Kernel) + PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 34s, sys: 396 ms, total: 3min 34s\n",
      "Wall time: 3min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "svm_clf_pca = SVC(C=1, gamma=0.001)\n",
    "svm_clf_pca.fit(X_train_pca, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2: Evaluate SVC (RBF Kernel) + PCA on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Accuracy:  0.9659\n",
      "\n",
      "Test Confusion Matrix:\n",
      "[[ 968    0    2    1    0    3    3    1    2    0]\n",
      " [   0 1126    3    0    0    1    3    0    2    0]\n",
      " [   6    2  993    3    2    0    1   14   10    1]\n",
      " [   0    0    2  984    1    7    0   10    6    0]\n",
      " [   1    0    8    0  945    2    4    7    2   13]\n",
      " [   2    0    1   12    3  854    7    5    7    1]\n",
      " [   6    2    1    0    4    9  930    2    4    0]\n",
      " [   0    7   17    3    1    1    0  986    0   13]\n",
      " [   3    0    4    9    6   12    3    9  926    2]\n",
      " [   4    6    4   12   17    2    0   14    3  947]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.99      0.98       980\n",
      "         1.0       0.99      0.99      0.99      1135\n",
      "         2.0       0.96      0.96      0.96      1032\n",
      "         3.0       0.96      0.97      0.97      1010\n",
      "         4.0       0.97      0.96      0.96       982\n",
      "         5.0       0.96      0.96      0.96       892\n",
      "         6.0       0.98      0.97      0.97       958\n",
      "         7.0       0.94      0.96      0.95      1028\n",
      "         8.0       0.96      0.95      0.96       974\n",
      "         9.0       0.97      0.94      0.95      1009\n",
      "\n",
      "   micro avg       0.97      0.97      0.97     10000\n",
      "   macro avg       0.97      0.97      0.97     10000\n",
      "weighted avg       0.97      0.97      0.97     10000\n",
      "\n",
      "CPU times: user 50.9 s, sys: 33.3 ms, total: 50.9 s\n",
      "Wall time: 50.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "y_test_predicted = svm_clf_pca.predict(X_test_pca)\n",
    "\n",
    "accuracy_score_test = np.mean(y_test_predicted == y_test)\n",
    "print(\"\\nTest Accuracy: \", accuracy_score_test)\n",
    "\n",
    "print(\"\\nTest Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_test_predicted))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_test_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 3: SVC (RBF Kernel) \n",
    "\n",
    "We experiment with the SVC (RBF Kernel) without applying dimensionaly reducion on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8min 17s, sys: 614 ms, total: 8min 17s\n",
      "Wall time: 8min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "svm_clf = SVC(gamma=0.001)\n",
    "svm_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 3: Evaluate SVC (RBF Kernel) on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy:  0.9657\n",
      "\n",
      "Test Confusion Matrix:\n",
      "[[ 968    0    2    1    0    3    3    1    2    0]\n",
      " [   0 1125    3    0    0    1    3    1    2    0]\n",
      " [   6    1  992    3    2    0    1   15   11    1]\n",
      " [   0    0    2  982    1    8    0   11    6    0]\n",
      " [   1    0    8    0  945    2    4    8    3   11]\n",
      " [   2    0    1   13    2  853    7    5    8    1]\n",
      " [   6    2    1    0    4    9  931    2    3    0]\n",
      " [   1    5   15    3    3    0    0  988    0   13]\n",
      " [   3    1    5    8    6   11    3   10  924    3]\n",
      " [   4    6    4   12   15    1    0   15    3  949]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.99      0.98       980\n",
      "         1.0       0.99      0.99      0.99      1135\n",
      "         2.0       0.96      0.96      0.96      1032\n",
      "         3.0       0.96      0.97      0.97      1010\n",
      "         4.0       0.97      0.96      0.96       982\n",
      "         5.0       0.96      0.96      0.96       892\n",
      "         6.0       0.98      0.97      0.97       958\n",
      "         7.0       0.94      0.96      0.95      1028\n",
      "         8.0       0.96      0.95      0.95       974\n",
      "         9.0       0.97      0.94      0.96      1009\n",
      "\n",
      "   micro avg       0.97      0.97      0.97     10000\n",
      "   macro avg       0.97      0.97      0.97     10000\n",
      "weighted avg       0.97      0.97      0.97     10000\n",
      "\n",
      "CPU times: user 2min 3s, sys: 82.2 ms, total: 2min 3s\n",
      "Wall time: 2min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "y_test_predicted = svm_clf.predict(X_test)\n",
    "\n",
    "accuracy_score_test = np.mean(y_test_predicted == y_test)\n",
    "print(\"\\nAccuracy: \", accuracy_score_test)\n",
    "\n",
    "\n",
    "print(\"\\nTest Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_test_predicted))\n",
    "\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_test_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 4: Logistic Regression (Softmax Regression) + PCA\n",
    "\n",
    "We use the best performing solver (i.e., lbfgs) from previous notebook to train the logistic regression model on the PCA transformed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 37.3 s, sys: 3.36 s, total: 40.7 s\n",
      "Wall time: 10.5 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hasan/anaconda/lib/python3.5/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "softmax_reg_pca = LogisticRegression(solver='lbfgs', multi_class='multinomial')\n",
    "\n",
    "softmax_reg_pca.fit(X_train_pca, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 4: Evaluate Softmax Regression + PCA on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of Iterations: [100]\n",
      "\n",
      "Test Accuracy:  0.9265\n",
      "\n",
      "Test Confusion Matrix:\n",
      "[[ 957    0    1    2    1    6    8    3    2    0]\n",
      " [   0 1114    3    2    0    1    3    2   10    0]\n",
      " [   7    5  931   17   12    3    9   11   34    3]\n",
      " [   3    3   18  919    1   22    3   11   23    7]\n",
      " [   1    2    8    2  917    0   10    4    9   29]\n",
      " [   7    5    3   33    8  778   13    6   35    4]\n",
      " [  12    3    8    2    6   12  912    1    2    0]\n",
      " [   0    9   29    5    6    1    0  948    0   30]\n",
      " [   6    6    6   22    9   24    7   11  874    9]\n",
      " [   9    7    2   10   25    7    0   26    8  915]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.98      0.97       980\n",
      "         1.0       0.97      0.98      0.97      1135\n",
      "         2.0       0.92      0.90      0.91      1032\n",
      "         3.0       0.91      0.91      0.91      1010\n",
      "         4.0       0.93      0.93      0.93       982\n",
      "         5.0       0.91      0.87      0.89       892\n",
      "         6.0       0.95      0.95      0.95       958\n",
      "         7.0       0.93      0.92      0.92      1028\n",
      "         8.0       0.88      0.90      0.89       974\n",
      "         9.0       0.92      0.91      0.91      1009\n",
      "\n",
      "   micro avg       0.93      0.93      0.93     10000\n",
      "   macro avg       0.93      0.93      0.93     10000\n",
      "weighted avg       0.93      0.93      0.93     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"No. of Iterations:\", softmax_reg_pca.n_iter_ )\n",
    "\n",
    "\n",
    "y_test_predicted = softmax_reg_pca.predict(X_test_pca)\n",
    "#print(y_test_predict)\n",
    "\n",
    "accuracy_score_test = np.mean(y_test_predicted == y_test)\n",
    "print(\"\\nTest Accuracy: \", accuracy_score_test)\n",
    "\n",
    "\n",
    "print(\"\\nTest Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_test_predicted))\n",
    "\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_test_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary of Results from 4 Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classifier</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Running-Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LinearSVC + PCA</td>\n",
       "      <td>0.9180</td>\n",
       "      <td>3min 33s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SVM(RBF) + PCA</td>\n",
       "      <td>0.9659</td>\n",
       "      <td>3min 34s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SVM(RBF)</td>\n",
       "      <td>0.9657</td>\n",
       "      <td>8min 17s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Softmax + PCA</td>\n",
       "      <td>0.9265</td>\n",
       "      <td>10.5 s</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Classifier  Accuracy Running-Time\n",
       "0  LinearSVC + PCA    0.9180     3min 33s\n",
       "1   SVM(RBF) + PCA    0.9659     3min 34s\n",
       "2         SVM(RBF)    0.9657     8min 17s\n",
       "3    Softmax + PCA    0.9265       10.5 s"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [[\"LinearSVC + PCA\", 0.918, \"3min 33s\"], \n",
    "        [\"SVM(RBF) + PCA\", 0.9659, \"3min 34s\"],\n",
    "        [\"SVM(RBF)\", 0.9657, \"8min 17s\"],\n",
    "        [\"Softmax + PCA\", 0.9265, \"10.5 s\"]]\n",
    "\n",
    "pd.DataFrame(data, columns=[\"Classifier\", \"Accuracy\", \"Running-Time\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparative Understanding\n",
    "\n",
    "We have done 4 experiments using SVM and Logistic Regression classifiers.\n",
    "\n",
    "The first 3 experiements are done using 2 SVM algorithms, with the effect of PCA.\n",
    "\n",
    "The experimental results confirm our hypotheses:\n",
    "- The kernelized SVM models will perform significantly better than the linear SVM model.\n",
    "- The RBF Kernel based SVM will perform better than Softmax regression classifier.\n",
    "- Dimensionaly reduction (by retaining maximum variance) should improve the performance.\n",
    "\n",
    "We make following observations.\n",
    "- The SVM classifiers perform **significantly** better than the Softmax classifier.\n",
    "- The SVM classifier training and prediction time is **longer**.\n",
    "- The RBF kernel based SVM classifier performs better than the linear SVM classifier. It indicates that for this non-linear image classsification problem the kernelized SVM is the most suitable algorithm.\n",
    "- Dimensionality reduction improves the performance slightly on the RBF kernel based SVM.\n",
    "\n",
    "### Thus, for image classification problems RBF kernel based SVM model should be used with dimensionality reduction."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
